{
  "components": {
    "comp-config-component": {
      "executorLabel": "exec-config-component",
      "outputDefinitions": {
        "parameters": {
          "bucket_name": {
            "parameterType": "STRING"
          },
          "dataset_id": {
            "parameterType": "STRING"
          },
          "display_name": {
            "parameterType": "STRING"
          },
          "endpoint_display_name": {
            "parameterType": "STRING"
          },
          "image_name": {
            "parameterType": "STRING"
          },
          "machine_type": {
            "parameterType": "STRING"
          },
          "model_display_name": {
            "parameterType": "STRING"
          },
          "model_tag": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "region": {
            "parameterType": "STRING"
          },
          "repository_name": {
            "parameterType": "STRING"
          },
          "service_account": {
            "parameterType": "STRING"
          },
          "table_id": {
            "parameterType": "STRING"
          },
          "tensorboard": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-fetch-data-from-bigquery": {
      "executorLabel": "exec-fetch-data-from-bigquery",
      "inputDefinitions": {
        "parameters": {
          "dataset_id": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "table_id": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "output_dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-train-model": {
      "executorLabel": "exec-train-model",
      "inputDefinitions": {
        "artifacts": {
          "input_dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "base_output_directory": {
            "defaultValue": "gs://dataset-bike-share/training_outputs/",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "display_name": {
            "defaultValue": "vertex-custom-training-job-cpu",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "enable_web_access": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "encryption_spec_key_name": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "labels": {
            "defaultValue": {},
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "location": {
            "defaultValue": "{{$.pipeline_google_cloud_location}}",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "max_wait_duration": {
            "defaultValue": "86400s",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "network": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "persistent_resource_id": {
            "defaultValue": "{{$.pipeline_persistent_resource_id}}",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project": {
            "defaultValue": "{{$.pipeline_google_cloud_project_id}}",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "reserved_ip_ranges": {
            "defaultValue": [],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "restart_job_on_worker_restart": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "service_account": {
            "defaultValue": "1014348685594-compute@developer.gserviceaccount.com",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "strategy": {
            "defaultValue": "STANDARD",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "tensorboard": {
            "defaultValue": "projects/1014348685594/locations/us-central1/tensorboards/5353069116850700288",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "timeout": {
            "defaultValue": "3600s",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "worker_pool_specs": {
            "defaultValue": [
              {
                "container_spec": {
                  "args": [
                    "--executor_input",
                    "{{$.json_escape[1]}}",
                    "--function_to_execute",
                    "train_model"
                  ],
                  "command": [
                    "sh",
                    "-c",
                    "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'accelerate==0.34.2' 'fastparquet==2024.5.0' 'pandas==2.2.2' 'pyarrow==17.0.0' 'scikit-learn==1.5.2' 'torch==2.4.1' 'transformers==4.44.2' 'tensorboard==2.15.0' && \"$0\" \"$@\"\n",
                    "sh",
                    "-ec",
                    "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
                    "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_model(\n    input_dataset: Input[Dataset],\n    output_model: Output[Model],\n    output_metrics: Output[Metrics],\n):\n    import joblib\n    import numpy as np\n    import pandas as pd\n    import torch\n    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n    from sklearn.preprocessing import LabelEncoder\n    from torch.utils.tensorboard import SummaryWriter\n    from pathlib import Path\n\n    device = torch.device(\"cpu\")  # \u2705 Force CPU\n    print(f\"\u2705 Using device: {device}\")\n\n    df = pd.read_parquet(input_dataset.path)\n    label_encoder = LabelEncoder()\n    labels = label_encoder.fit_transform(df[\"label\"])\n    num_labels = len(np.unique(labels))\n\n    texts = df[\"text\"].tolist()\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        texts, labels, test_size=0.2, stratify=labels\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n    model.to(device)\n\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n\n    class TextDataset(torch.utils.data.Dataset):\n        def __init__(self, encodings, labels):\n            self.encodings = encodings\n            self.labels = labels\n        def __getitem__(self, idx):\n            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n            return item\n        def __len__(self):\n            return len(self.labels)\n\n    train_dataset = TextDataset(train_encodings, train_labels)\n    val_dataset = TextDataset(val_encodings, val_labels)\n\n    log_dir = str(Path(output_model.path) / \"logs\")\n    writer = SummaryWriter(log_dir)\n\n    training_args = TrainingArguments(\n        output_dir=str(Path(output_model.path) / \"results\"),\n        num_train_epochs=5,\n        per_device_train_batch_size=32,\n        evaluation_strategy=\"epoch\",\n        logging_dir=log_dir,\n        logging_steps=10,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n    trainer.train()\n\n    predictions = trainer.predict(val_dataset)\n    preds = predictions.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, preds, average=\"macro\")\n    accuracy = accuracy_score(val_labels, preds)\n\n    output_metrics.log_metric(\"accuracy\", accuracy)\n    writer.add_scalar(\"Accuracy\", accuracy)\n    writer.add_scalar(\"Precision\", precision)\n    writer.add_scalar(\"Recall\", recall)\n    writer.add_scalar(\"F1\", f1)\n\n    model.save_pretrained(output_model.path)\n    tokenizer.save_pretrained(output_model.path)\n    writer.close()\n    joblib.dump(label_encoder, f\"{output_model.path}/label_encoder.joblib\")\n\n    print(f\"\u2705 Training complete. Model saved to {output_model.path}\")\n\n"
                  ],
                  "env": [],
                  "image_uri": "python:3.12"
                },
                "disk_spec": {
                  "boot_disk_size_gb": 100.0,
                  "boot_disk_type": "pd-ssd"
                },
                "machine_spec": {
                  "machine_type": "n1-standard-4"
                },
                "replica_count": 1.0
              }
            ],
            "isOptional": true,
            "parameterType": "LIST"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "output_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "output_model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "gcp_resources": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-config-component": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "config_component"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef config_component() -> NamedTuple(\n    \"ConfigOutputs\",\n    [\n        (\"project_id\", str),\n        (\"region\", str),\n        (\"bucket_name\", str),\n        (\"dataset_id\", str),\n        (\"table_id\", str),\n        (\"service_account\", str),\n        (\"repository_name\", str),\n        (\"image_name\", str),\n        (\"model_tag\", str),\n        (\"model_display_name\", str),\n        (\"endpoint_display_name\", str),\n        (\"tensorboard\", str),  # \u2705 Added TensorBoard Path\n        (\"display_name\", str),\n        (\"machine_type\", str),\n\n\n\n    ],\n):\n    from collections import namedtuple\n    outputs = namedtuple(\n        \"ConfigOutputs\",\n        [\n            \"project_id\",\n            \"region\",\n            \"bucket_name\",\n            \"dataset_id\",\n            \"table_id\",\n            \"service_account\",\n            \"repository_name\",\n            \"image_name\",\n            \"model_tag\",\n            \"model_display_name\",\n            \"endpoint_display_name\",\n            \"tensorboard\",\n            \"display_name\",\n            \"machine_type\",  # \u2705 Added Display Name and Machine Type\n        ],\n    )\n\n    return outputs(\n        \"rock-flag-452514-v8\",\n        \"us-central1\",\n        \"dataset-bike-share\",\n        \"vertexai_test_dataset\",\n        \"text_classification_data\",\n        \"1014348685594-compute@developer.gserviceaccount.com\",\n        \"repo-vertexai\",\n        \"text-classification-model\",\n        \"v1\",\n        \"text-classification-model\",\n        \"text-classification-endpoint\",\n        \"projects/1014348685594/locations/us-central1/tensorboards/5353069116850700288\",\n        \"vertex-custom-training-job-cpu-tensorboard\",\n        \"n1-standard-4\",  # \u2705 Added Machine Type\n    )\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-fetch-data-from-bigquery": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "fetch_data_from_bigquery"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'db-dtypes==1.3.0' 'google-cloud-bigquery==3.25.0' 'pandas==2.2.2' 'pyarrow==17.0.0' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef fetch_data_from_bigquery(\n    project_id: str,\n    dataset_id: str,\n    table_id: str,\n    output_dataset: Output[Dataset],\n):\n    from google.cloud import bigquery\n    import datetime\n\n    start_timestamp = (datetime.datetime.now() - datetime.timedelta(days=2)).strftime(\"%Y-%m-%d %H:%M:%S\")\n    query = f\"\"\"\n        SELECT text, label\n        FROM `{project_id}.{dataset_id}.{table_id}`\n        WHERE timestamp >= '{start_timestamp}'\n    \"\"\"\n\n    client = bigquery.Client(project=project_id)\n    df = client.query(query).to_dataframe()\n    print(f\"\u2705 Number of rows fetched: {len(df)}\")\n    df.to_parquet(output_dataset.path)\n\n"
          ],
          "image": "python:3.12"
        }
      },
      "exec-train-model": {
        "container": {
          "args": [
            "--type",
            "CustomJob",
            "--payload",
            "{\"display_name\": \"{{$.inputs.parameters['display_name']}}\", \"job_spec\": {\"worker_pool_specs\": {{$.inputs.parameters['worker_pool_specs']}}, \"scheduling\": {\"timeout\": \"{{$.inputs.parameters['timeout']}}\", \"restart_job_on_worker_restart\": {{$.inputs.parameters['restart_job_on_worker_restart']}}, \"strategy\": \"{{$.inputs.parameters['strategy']}}\", \"max_wait_duration\": \"{{$.inputs.parameters['max_wait_duration']}}\"}, \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"enable_web_access\": {{$.inputs.parameters['enable_web_access']}}, \"network\": \"{{$.inputs.parameters['network']}}\", \"reserved_ip_ranges\": {{$.inputs.parameters['reserved_ip_ranges']}}, \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}, \"persistent_resource_id\": \"{{$.inputs.parameters['persistent_resource_id']}}\"}, \"labels\": {{$.inputs.parameters['labels']}}, \"encryption_spec\": {\"kms_key_name\": \"{{$.inputs.parameters['encryption_spec_key_name']}}\"}}",
            "--project",
            "{{$.inputs.parameters['project']}}",
            "--location",
            "{{$.inputs.parameters['location']}}",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.custom_job.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.19.0"
        }
      }
    }
  },
  "pipelineInfo": {
    "name": "fetch-data-and-custom-train-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "config-component": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-config-component"
          },
          "taskInfo": {
            "name": "config-component"
          }
        },
        "fetch-data-from-bigquery": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-fetch-data-from-bigquery"
          },
          "dependentTasks": [
            "config-component"
          ],
          "inputs": {
            "parameters": {
              "dataset_id": {
                "taskOutputParameter": {
                  "outputParameterKey": "dataset_id",
                  "producerTask": "config-component"
                }
              },
              "project_id": {
                "taskOutputParameter": {
                  "outputParameterKey": "project_id",
                  "producerTask": "config-component"
                }
              },
              "table_id": {
                "taskOutputParameter": {
                  "outputParameterKey": "table_id",
                  "producerTask": "config-component"
                }
              }
            }
          },
          "taskInfo": {
            "name": "fetch-data-from-bigquery"
          }
        },
        "train-model": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-train-model"
          },
          "dependentTasks": [
            "fetch-data-from-bigquery"
          ],
          "inputs": {
            "artifacts": {
              "input_dataset": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "output_dataset",
                  "producerTask": "fetch-data-from-bigquery"
                }
              }
            }
          },
          "taskInfo": {
            "name": "train-model"
          }
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.10.1"
}